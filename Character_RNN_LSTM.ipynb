{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level LSTM in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('data/data.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 23, 71, 65, 28, 76, 57, 24, 13,  3,  3,  3, 66, 71, 65, 65, 49,\n",
       "       24, 38, 71, 64, 70, 44, 70, 76,  6, 24, 71, 57, 76, 24, 71, 44, 44,\n",
       "       24, 71, 44, 70, 68, 76, 55, 24, 76, 15, 76, 57, 49, 24, 78, 14, 23,\n",
       "       71, 65, 65, 49, 24, 38, 71, 64, 70, 44, 49, 24, 70,  6, 24, 78, 14,\n",
       "       23, 71, 65, 65, 49, 24, 70, 14, 24, 70, 28,  6, 24, 47, 17, 14,  3,\n",
       "       17, 71, 49, 16,  3,  3, 75, 15, 76, 57, 49, 28, 23, 70, 14])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as LSTM takes the input as one hot\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[ 0 23 71 65 28 76 57 24 13  3]\n",
      " [ 6 47 14 24 28 23 71 28 24 71]\n",
      " [76 14 77 24 47 57 24 71 24 38]\n",
      " [ 6 24 28 23 76 24 62 23 70 76]\n",
      " [24  6 71 17 24 23 76 57 24 28]\n",
      " [62 78  6  6 70 47 14 24 71 14]\n",
      " [24 21 14 14 71 24 23 71 77 24]\n",
      " [63 54 44 47 14  6 68 49 16 24]]\n",
      "\n",
      "y\n",
      " [[23 71 65 28 76 57 24 13  3  3]\n",
      " [47 14 24 28 23 71 28 24 71 28]\n",
      " [14 77 24 47 57 24 71 24 38 47]\n",
      " [24 28 23 76 24 62 23 70 76 38]\n",
      " [ 6 71 17 24 23 76 57 24 28 76]\n",
      " [78  6  6 70 47 14 24 71 14 77]\n",
      " [21 14 14 71 24 23 71 77 24  6]\n",
      " [54 44 47 14  6 68 49 16 24 11]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the network with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5... Step: 10... Loss: 3.2467... Val Loss: 3.1755\n",
      "Epoch: 1/5... Step: 20... Loss: 3.1358... Val Loss: 3.1310\n",
      "Epoch: 1/5... Step: 30... Loss: 3.1427... Val Loss: 3.1214\n",
      "Epoch: 1/5... Step: 40... Loss: 3.1103... Val Loss: 3.1192\n",
      "Epoch: 1/5... Step: 50... Loss: 3.1402... Val Loss: 3.1173\n",
      "Epoch: 1/5... Step: 60... Loss: 3.1153... Val Loss: 3.1141\n",
      "Epoch: 1/5... Step: 70... Loss: 3.1043... Val Loss: 3.1110\n",
      "Epoch: 1/5... Step: 80... Loss: 3.1119... Val Loss: 3.1019\n",
      "Epoch: 1/5... Step: 90... Loss: 3.0955... Val Loss: 3.0791\n",
      "Epoch: 1/5... Step: 100... Loss: 3.0451... Val Loss: 3.0258\n",
      "Epoch: 1/5... Step: 110... Loss: 2.9552... Val Loss: 2.9273\n",
      "Epoch: 1/5... Step: 120... Loss: 2.8439... Val Loss: 2.8299\n",
      "Epoch: 1/5... Step: 130... Loss: 2.7381... Val Loss: 2.6921\n",
      "Epoch: 2/5... Step: 140... Loss: 2.6473... Val Loss: 2.6085\n",
      "Epoch: 2/5... Step: 150... Loss: 2.5784... Val Loss: 2.5399\n",
      "Epoch: 2/5... Step: 160... Loss: 2.5260... Val Loss: 2.4858\n",
      "Epoch: 2/5... Step: 170... Loss: 2.4568... Val Loss: 2.4458\n",
      "Epoch: 2/5... Step: 180... Loss: 2.4327... Val Loss: 2.4109\n",
      "Epoch: 2/5... Step: 190... Loss: 2.3836... Val Loss: 2.3808\n",
      "Epoch: 2/5... Step: 200... Loss: 2.3686... Val Loss: 2.3470\n",
      "Epoch: 2/5... Step: 210... Loss: 2.3462... Val Loss: 2.3149\n",
      "Epoch: 2/5... Step: 220... Loss: 2.3029... Val Loss: 2.2841\n",
      "Epoch: 2/5... Step: 230... Loss: 2.2877... Val Loss: 2.2755\n",
      "Epoch: 2/5... Step: 240... Loss: 2.2679... Val Loss: 2.2368\n",
      "Epoch: 2/5... Step: 250... Loss: 2.2000... Val Loss: 2.2042\n",
      "Epoch: 2/5... Step: 260... Loss: 2.1774... Val Loss: 2.1775\n",
      "Epoch: 2/5... Step: 270... Loss: 2.1972... Val Loss: 2.1729\n",
      "Epoch: 3/5... Step: 280... Loss: 2.1906... Val Loss: 2.1444\n",
      "Epoch: 3/5... Step: 290... Loss: 2.1428... Val Loss: 2.1170\n",
      "Epoch: 3/5... Step: 300... Loss: 2.1244... Val Loss: 2.0976\n",
      "Epoch: 3/5... Step: 310... Loss: 2.0993... Val Loss: 2.0800\n",
      "Epoch: 3/5... Step: 320... Loss: 2.0662... Val Loss: 2.0584\n",
      "Epoch: 3/5... Step: 330... Loss: 2.0480... Val Loss: 2.0404\n",
      "Epoch: 3/5... Step: 340... Loss: 2.0644... Val Loss: 2.0235\n",
      "Epoch: 3/5... Step: 350... Loss: 2.0436... Val Loss: 2.0079\n",
      "Epoch: 3/5... Step: 360... Loss: 1.9712... Val Loss: 1.9889\n",
      "Epoch: 3/5... Step: 370... Loss: 2.0032... Val Loss: 1.9717\n",
      "Epoch: 3/5... Step: 380... Loss: 1.9722... Val Loss: 1.9570\n",
      "Epoch: 3/5... Step: 390... Loss: 1.9470... Val Loss: 1.9433\n",
      "Epoch: 3/5... Step: 400... Loss: 1.9257... Val Loss: 1.9248\n",
      "Epoch: 3/5... Step: 410... Loss: 1.9451... Val Loss: 1.9106\n",
      "Epoch: 4/5... Step: 420... Loss: 1.9273... Val Loss: 1.8970\n",
      "Epoch: 4/5... Step: 430... Loss: 1.9177... Val Loss: 1.8813\n",
      "Epoch: 4/5... Step: 440... Loss: 1.8986... Val Loss: 1.8726\n",
      "Epoch: 4/5... Step: 450... Loss: 1.8408... Val Loss: 1.8556\n",
      "Epoch: 4/5... Step: 460... Loss: 1.8236... Val Loss: 1.8456\n",
      "Epoch: 4/5... Step: 470... Loss: 1.8644... Val Loss: 1.8367\n",
      "Epoch: 4/5... Step: 480... Loss: 1.8422... Val Loss: 1.8251\n",
      "Epoch: 4/5... Step: 490... Loss: 1.8501... Val Loss: 1.8099\n",
      "Epoch: 4/5... Step: 500... Loss: 1.8488... Val Loss: 1.8029\n",
      "Epoch: 4/5... Step: 510... Loss: 1.8131... Val Loss: 1.7895\n",
      "Epoch: 4/5... Step: 520... Loss: 1.8293... Val Loss: 1.7816\n",
      "Epoch: 4/5... Step: 530... Loss: 1.7912... Val Loss: 1.7703\n",
      "Epoch: 4/5... Step: 540... Loss: 1.7460... Val Loss: 1.7571\n",
      "Epoch: 4/5... Step: 550... Loss: 1.8034... Val Loss: 1.7487\n",
      "Epoch: 5/5... Step: 560... Loss: 1.7698... Val Loss: 1.7358\n",
      "Epoch: 5/5... Step: 570... Loss: 1.7498... Val Loss: 1.7279\n",
      "Epoch: 5/5... Step: 580... Loss: 1.7271... Val Loss: 1.7180\n",
      "Epoch: 5/5... Step: 590... Loss: 1.7291... Val Loss: 1.7094\n",
      "Epoch: 5/5... Step: 600... Loss: 1.7167... Val Loss: 1.7059\n",
      "Epoch: 5/5... Step: 610... Loss: 1.7024... Val Loss: 1.7007\n",
      "Epoch: 5/5... Step: 620... Loss: 1.7136... Val Loss: 1.6897\n",
      "Epoch: 5/5... Step: 630... Loss: 1.7201... Val Loss: 1.6809\n",
      "Epoch: 5/5... Step: 640... Loss: 1.6917... Val Loss: 1.6737\n",
      "Epoch: 5/5... Step: 650... Loss: 1.6720... Val Loss: 1.6613\n",
      "Epoch: 5/5... Step: 660... Loss: 1.6592... Val Loss: 1.6609\n",
      "Epoch: 5/5... Step: 670... Loss: 1.6857... Val Loss: 1.6581\n",
      "Epoch: 5/5... Step: 680... Loss: 1.6745... Val Loss: 1.6499\n",
      "Epoch: 5/5... Step: 690... Loss: 1.6509... Val Loss: 1.6400\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 5 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs , batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions and top k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating text\n",
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annat, as the possing, and his sack of the contoration to the fithing wher a conterarest to a gried to havone and the sead in the ferert whaters thrute had betore to sitther, and heard to the some offer the for think trat to sinces with the conderted on his hound, and she thought he saw her and when the chains stook to her, and how way hadibe it heared that still no\n",
      "houre had been thought on his firth of too walking that the most still\n",
      "stried of him.\n",
      "\n",
      "\"In store that's not,\" she said.\n",
      "\n",
      "That was and alres and have to seill have to see humbands the charted that the stook as so the same. She were betingering of her, what at her herself with his would be have the for and the poon of a showe one, that so consterstant with the chander that that she she thought all\n",
      "her trention and hersalf, taken that he had\n",
      "been do that at her shunt her. He cauned her hand, and to been haddens and shided happeranges would to thow he sont has stall that to\n",
      "the\n",
      "mother, and the counded him. \"I cond to she had be a co\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said to all him. \"I've take the mome, and I delight to\n",
      "see in that her all sees, I dad't contenst. Her's state to to an if the mush to the sartice and\n",
      "seminise.\"\n",
      "\n",
      "And the strunge, and to saeme the conters of the prince was stild by her, and so dis not talkilg, this, and he said of his heant, whene he say to still an in one the same\n",
      "and stoption, and the sone waits the saigh that had to harded the same and with the pissact, the position of his hapsed what though there had the pincess was so doon. The\n",
      "sacter as hume him on the carest.\n",
      "\n",
      "\"Went, is we hor's she to be the prain of it as is should, as he had not a since. He had that imered to me on the seal and thin in some and me one of this\n",
      "ary of improped. It that it and who was at it's but a shall to to som him. And to his\n",
      "samp as she was ston to me to her some that\n",
      "the carse at and the poolly of the\n",
      "pasting on. The course, and he said.\n",
      "\n",
      "Stepan Arkadyevitch sout his bet in ster of the mase this hould be ond\n",
      "sinchion that he carest on the well of to\n",
      "cleak of the said\n",
      "wite\n",
      "a monders.\n",
      "\n",
      "\"The possion of a share it should sen's\n",
      "as a cances,\" said Levin hid and\n",
      "had\n",
      "not the countriace, were taken the comprestions, and had stall, asterdading in the candrast. To the port was to the stood. \"It thouth, im, I had at at the\n",
      "converseast. And the she that I and he hands and she said this simple to the concincoss to alr the strean...\"\n",
      "\n",
      "\"Yes in the condition....\"\n",
      "\n",
      "He sood is the pastion of which they the\n",
      "forth to\n",
      "side, with hought all the\n",
      "parsaciar houst a strouts. The\n",
      "posiness there was\n",
      "suck and wall this sould. And the\n",
      "strack of the\n",
      "pearense who has he husted.\n",
      "\n",
      "\"Way to mure said in, as his caret a lothing of happants of the pinss to home went.\"\n",
      "\n",
      "\"They, I's should not terrend, and samay.\"\n",
      "\n",
      "\"That was still made, but I's to be all the commints. Will the streak about, as a cance, than shook trate, it seat the came, and he som ous a clurt as showe to as he day to seeme in in the pattion, and he dound on a\n",
      "clestion fartion. And this so thit, the\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levin asked of her suppents of his farse, who had to her showed as, thought to said in it\n",
      "a pecticulter some a prothing. The farter than taking of in she who would be and he hid brongirg the parss of thore at the hapter and there was a stringe of herserf, he come to the porinice that. And he was say, and the monting of would not the pose, they, and thene were sharith them. Stry had not chere the sace, whone so her south, and all the\n",
      "ridel from and to to sind his wean of the condiculed to his fartere.\n",
      "\n",
      "So had stopped and this. He thise has begain the praith which that she showed home take to his conterstation whice she was not a posering, but the cancial, this hussed.\n",
      "\n",
      "Annt sat is the\n",
      "sear, and and she was the begine and which he was had been\n",
      "at\n",
      "the bighers to the\n",
      "chair of the stift and the seemis at him a dreash, worls over to\n",
      "the sumbreas ought his head, and stow in hom to a chinge out, which the carting was at offor to the frighed would be such on the sear, the\n",
      "froncersing with the same to her and went on and with\n",
      "the sace of the stacitiar that she was she was have so she, that werk him and seening on it the\n",
      "prince what he well began a coms, at his him anster humpering.\n",
      "\n",
      "\"I can it a migution, and that, that stees of his fron it is,\" she said, but to the\n",
      "secaning of a placticase\n",
      "time to be shool to\n",
      "to him to her happing, had to the\n",
      "cortriation, as this sawing of the cannies, to bage as had brenely the partens, and toon with a thraighters that\n",
      "all at his\n",
      "foomer.\n",
      "\n",
      "The sarding at the\n",
      "fall to her that where he show have began the fact hushers, had been and she was stalled of the poors that he han\n",
      "has see in his fron thinks, shorit and\n",
      "who had not be the pertor whome was and whith he shad strued him a stick out to but\n",
      "and time of the mome that she said to the\n",
      "preation of the stor and and\n",
      "he went\n",
      "on\n",
      "her a plich fight at he houghts, and her sawes, and then horse wing antthang.\n",
      "\n",
      "\"Whete is same. What? It the courden's shand to him.\n",
      "\n",
      "\"Oh, yes at the mast, I so be impicted that it all the facule. How shive you so the book of him in out at the chiscesss. It sounst houre.\n",
      "\n",
      "\"Where you me hand a straight on the coulded bet in the stall. There sones our is stonger out there words to be inthose,\n",
      "and he could, at\n",
      "it in her and to him his\n",
      "sector... And\n",
      "the carts to she\n",
      "was some though has at the saighing of them that she was to have he wastended of his face the seed,\n",
      "and would not but as he was the sames, were an of it.\n",
      "\n",
      "\"I'm not to\n",
      "depart of her there with that. I'm a lang as the face\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 2500, top_k=5, prime=\"Levin asked\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
